{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1715dd42-a7b6-4369-8244-0e4ae0b4af1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI set to: https://dagshub.com/api/v1/repo-buckets/s3/sandhya-bdb\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as sandhya-bdb\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as sandhya-bdb\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"sandhya-bdb/mlflow_dagshub_new\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"sandhya-bdb/mlflow_dagshub_new\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository sandhya-bdb/mlflow_dagshub_new initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository sandhya-bdb/mlflow_dagshub_new initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DagsHub and MLflow initialized successfully.\n",
      "Dataset 'survey_results_op.csv' loaded successfully. Shape: (29956, 20)\n",
      "First 5 rows of the dataset:\n",
      "  respondent_id gender  zone            occupation  income_levels  \\\n",
      "0        R00001      M     3  Working Professional              1   \n",
      "1        R00002      F     4  Working Professional              5   \n",
      "2        R00003      F     1  Working Professional              5   \n",
      "3        R00004      F     3  Working Professional              3   \n",
      "4        R00005      M     4               Student              0   \n",
      "\n",
      "   consume_frequency(weekly) current_brand preferable_consumption_size  \\\n",
      "0                          2      Newcomer             Medium (500 ml)   \n",
      "1                          3   Established             Medium (500 ml)   \n",
      "2                          2      Newcomer             Medium (500 ml)   \n",
      "3                          3      Newcomer             Medium (500 ml)   \n",
      "4                          2   Established             Medium (500 ml)   \n",
      "\n",
      "   awareness_of_other_brands reasons_for_choosing_brands flavor_preference  \\\n",
      "0                          1                       Price       Traditional   \n",
      "1                          2                     Quality            Exotic   \n",
      "2                          2                Availability       Traditional   \n",
      "3                          1            Brand Reputation            Exotic   \n",
      "4                          1                Availability       Traditional   \n",
      "\n",
      "  purchase_channel packaging_preference                       health_concerns  \\\n",
      "0           Online               Simple  Medium (Moderately health-conscious)   \n",
      "1     Retail Store              Premium  Medium (Moderately health-conscious)   \n",
      "2     Retail Store              Premium  Medium (Moderately health-conscious)   \n",
      "3           Online         Eco-Friendly              Low (Not very concerned)   \n",
      "4           Online              Premium  Medium (Moderately health-conscious)   \n",
      "\n",
      "  typical_consumption_situations price_range age_group  cf_ab_score  \\\n",
      "0       Active (eg. Sports, gym)     100-150     26-35         0.67   \n",
      "1           Social (eg. Parties)     200-250     46-55         0.60   \n",
      "2       Active (eg. Sports, gym)     200-250     36-45         0.50   \n",
      "3       Active (eg. Sports, gym)     150-200     26-35         0.75   \n",
      "4       Active (eg. Sports, gym)      50-100     18-25         0.67   \n",
      "\n",
      "   zas_score  bsi  \n",
      "0          3    1  \n",
      "1         20    0  \n",
      "2          5    0  \n",
      "3          9    0  \n",
      "4          0    0  \n",
      "\n",
      "Features shape: (29956, 18), Target shape: (29956,)\n",
      "\n",
      "Columns for Label Encoding: ['age_group', 'income_levels', 'health_concerns', 'consume_frequency(weekly)', 'preferable_consumption_size']\n",
      "Columns for One-Hot Encoding: ['gender', 'occupation', 'current_brand', 'reasons_for_choosing_brands', 'flavor_preference', 'purchase_channel', 'packaging_preference', 'typical_consumption_situations']\n",
      "Label encoded 'age_group'. Classes: ['18-25' '26-35' '36-45' '46-55' '56-70']\n",
      "Label encoded 'income_levels'. Classes: [0 1 2 3 4 5]\n",
      "Label encoded 'health_concerns'. Classes: ['High (Very health-conscious)' 'Low (Not very concerned)'\n",
      " 'Medium (Moderately health-conscious)']\n",
      "Label encoded 'consume_frequency(weekly)'. Classes: [1 2 3]\n",
      "Label encoded 'preferable_consumption_size'. Classes: ['Large (1 L)' 'Medium (500 ml)' 'Small (250 ml)']\n",
      "\n",
      "Applied One-Hot Encoding. New X shape: (29956, 24)\n",
      "Converted boolean column 'gender_M' to int.\n",
      "Converted boolean column 'occupation_Retired' to int.\n",
      "Converted boolean column 'occupation_Student' to int.\n",
      "Converted boolean column 'occupation_Working Professional' to int.\n",
      "Converted boolean column 'current_brand_Newcomer' to int.\n",
      "Converted boolean column 'reasons_for_choosing_brands_Brand Reputation' to int.\n",
      "Converted boolean column 'reasons_for_choosing_brands_Price' to int.\n",
      "Converted boolean column 'reasons_for_choosing_brands_Quality' to int.\n",
      "Converted boolean column 'flavor_preference_Traditional' to int.\n",
      "Converted boolean column 'purchase_channel_Retail Store' to int.\n",
      "Converted boolean column 'packaging_preference_Premium' to int.\n",
      "Converted boolean column 'packaging_preference_Simple' to int.\n",
      "Converted boolean column 'typical_consumption_situations_Casual (eg. At home)' to int.\n",
      "Converted boolean column 'typical_consumption_situations_Social (eg. Parties)' to int.\n",
      "\n",
      "Target variable 'price_range' encoded. Original classes: ['100-150' '150-200' '200-250' '50-100']\n",
      "Data split into training and testing sets.\n",
      "Training set features shape: (22467, 24)\n",
      "Testing set features shape: (7489, 24)\n",
      "\n",
      "--- Starting Model Training and Evaluation ---\n",
      "\n",
      "Training Gaussian Naive Bayes...\n",
      "MLflow Run ID: 5c9ca4a1974040138ae41f43bd40dd6d\n",
      "  Accuracy: 0.5662\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.25      0.32      1930\n",
      "           1       0.59      0.34      0.43      2223\n",
      "           2       0.70      0.89      0.78      2430\n",
      "           3       0.42      0.92      0.58       906\n",
      "\n",
      "    accuracy                           0.57      7489\n",
      "   macro avg       0.54      0.60      0.53      7489\n",
      "weighted avg       0.57      0.57      0.53      7489\n",
      "\n",
      "------------------------------\n",
      "ðŸƒ View run Train_Gaussian Naive Bayes at: https://dagshub.com/sandhya-bdb/mlflow_dagshub_new.mlflow/#/experiments/0/runs/5c9ca4a1974040138ae41f43bd40dd6d\n",
      "ðŸ§ª View experiment at: https://dagshub.com/sandhya-bdb/mlflow_dagshub_new.mlflow/#/experiments/0\n",
      "\n",
      "Training Logistic Regression...\n",
      "MLflow Run ID: a1b9366b32fe43bda8d42e0696d97d0b\n",
      "  Accuracy: 0.7937\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74      1930\n",
      "           1       0.74      0.76      0.75      2223\n",
      "           2       0.89      0.89      0.89      2430\n",
      "           3       0.79      0.74      0.76       906\n",
      "\n",
      "    accuracy                           0.79      7489\n",
      "   macro avg       0.79      0.78      0.79      7489\n",
      "weighted avg       0.79      0.79      0.79      7489\n",
      "\n",
      "------------------------------\n",
      "ðŸƒ View run Train_Logistic Regression at: https://dagshub.com/sandhya-bdb/mlflow_dagshub_new.mlflow/#/experiments/0/runs/a1b9366b32fe43bda8d42e0696d97d0b\n",
      "ðŸ§ª View experiment at: https://dagshub.com/sandhya-bdb/mlflow_dagshub_new.mlflow/#/experiments/0\n",
      "\n",
      "Training Support Vector Machine (SVM)...\n",
      "MLflow Run ID: 6e19fc7167db483e9e051aa80ee754c0\n",
      "  Accuracy: 0.8129\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.77      0.76      1930\n",
      "           1       0.76      0.79      0.78      2223\n",
      "           2       0.91      0.90      0.90      2430\n",
      "           3       0.82      0.74      0.78       906\n",
      "\n",
      "    accuracy                           0.81      7489\n",
      "   macro avg       0.81      0.80      0.80      7489\n",
      "weighted avg       0.81      0.81      0.81      7489\n",
      "\n",
      "------------------------------\n",
      "ðŸƒ View run Train_Support Vector Machine (SVM) at: https://dagshub.com/sandhya-bdb/mlflow_dagshub_new.mlflow/#/experiments/0/runs/6e19fc7167db483e9e051aa80ee754c0\n",
      "ðŸ§ª View experiment at: https://dagshub.com/sandhya-bdb/mlflow_dagshub_new.mlflow/#/experiments/0\n",
      "\n",
      "Training Random Forest...\n",
      "MLflow Run ID: b9259dd578dd4efcb9d030022cc32865\n",
      "  Accuracy: 0.8966\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88      1930\n",
      "           1       0.85      0.89      0.87      2223\n",
      "           2       0.94      0.93      0.94      2430\n",
      "           3       0.92      0.89      0.90       906\n",
      "\n",
      "    accuracy                           0.90      7489\n",
      "   macro avg       0.90      0.89      0.90      7489\n",
      "weighted avg       0.90      0.90      0.90      7489\n",
      "\n",
      "------------------------------\n",
      "ðŸƒ View run Train_Random Forest at: https://dagshub.com/sandhya-bdb/mlflow_dagshub_new.mlflow/#/experiments/0/runs/b9259dd578dd4efcb9d030022cc32865\n",
      "ðŸ§ª View experiment at: https://dagshub.com/sandhya-bdb/mlflow_dagshub_new.mlflow/#/experiments/0\n",
      "\n",
      "Training XGBoost...\n",
      "MLflow Run ID: 2b5eaa271efc4b22a8e3f82975661af0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:01:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.9246\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91      1930\n",
      "           1       0.90      0.91      0.91      2223\n",
      "           2       0.96      0.95      0.95      2430\n",
      "           3       0.92      0.92      0.92       906\n",
      "\n",
      "    accuracy                           0.92      7489\n",
      "   macro avg       0.92      0.92      0.92      7489\n",
      "weighted avg       0.92      0.92      0.92      7489\n",
      "\n",
      "------------------------------\n",
      "ðŸƒ View run Train_XGBoost at: https://dagshub.com/sandhya-bdb/mlflow_dagshub_new.mlflow/#/experiments/0/runs/2b5eaa271efc4b22a8e3f82975661af0\n",
      "ðŸ§ª View experiment at: https://dagshub.com/sandhya-bdb/mlflow_dagshub_new.mlflow/#/experiments/0\n",
      "\n",
      "Training Light Gradient Boosting Machine (LightGBM)...\n",
      "MLflow Run ID: f03e7d5885534cc7b7ca62bec59b1c60\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 81\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 24\n",
      "[LightGBM] [Info] Start training from score -1.343386\n",
      "[LightGBM] [Info] Start training from score -1.228925\n",
      "[LightGBM] [Info] Start training from score -1.126779\n",
      "[LightGBM] [Info] Start training from score -2.100810\n",
      "  Accuracy: 0.9267\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91      1930\n",
      "           1       0.90      0.92      0.91      2223\n",
      "           2       0.96      0.96      0.96      2430\n",
      "           3       0.92      0.92      0.92       906\n",
      "\n",
      "    accuracy                           0.93      7489\n",
      "   macro avg       0.92      0.92      0.92      7489\n",
      "weighted avg       0.93      0.93      0.93      7489\n",
      "\n",
      "------------------------------\n",
      "ðŸƒ View run Train_Light Gradient Boosting Machine (LightGBM) at: https://dagshub.com/sandhya-bdb/mlflow_dagshub_new.mlflow/#/experiments/0/runs/f03e7d5885534cc7b7ca62bec59b1c60\n",
      "ðŸ§ª View experiment at: https://dagshub.com/sandhya-bdb/mlflow_dagshub_new.mlflow/#/experiments/0\n",
      "\n",
      "--- Model Training and Evaluation Complete ---\n",
      "Best model found: 'Light Gradient Boosting Machine (LightGBM)' with accuracy 0.9267 (MLflow Run ID: f03e7d5885534cc7b7ca62bec59b1c60)\n",
      "\n",
      "--- Saving Best Model and Preprocessing Artifacts ---\n",
      "Best model ('Light Gradient Boosting Machine (LightGBM)') saved to 'best_price_range_model.pkl'\n",
      "Target encoder saved to 'price_range_target_encoder.pkl'\n",
      "Label encoders dictionary saved to 'label_encoders.pkl'\n",
      "Logging saved artifacts to MLflow run ID: f03e7d5885534cc7b7ca62bec59b1c60\n",
      "Saved artifacts logged as MLflow artifacts.\n",
      "\n",
      "MLflow tracking and artifact saving complete.\n",
      "View results in MLflow UI by running 'mlflow ui' in your terminal.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # End-to-End ML Workflow: Price Range Prediction\n",
    "#\n",
    "# This notebook performs the following steps:\n",
    "# 1. Loads the dataset.\n",
    "# 2. Preprocesses features and the target variable.\n",
    "# 3. Splits data into training and testing sets.\n",
    "# 4. Trains multiple machine learning models.\n",
    "# 5. Logs parameters, metrics, and classification reports to MLflow.\n",
    "# 6. Identifies the best performing model based on accuracy.\n",
    "# 7. **Saves the best model and all necessary preprocessing artifacts (encoders) to .pkl files.**\n",
    "#\n",
    "# After running this notebook, you will have `.pkl` files ready to be used by a Streamlit application for inference.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup and Configuration\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import mlflow\n",
    "import dagshub\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# --- MLflow and DagsHub Configuration ---\n",
    "# IMPORTANT: Set these environment variables in your terminal BEFORE running the notebook\n",
    "# export DAGSHUB_USER=\"your_username\"\n",
    "# export DAGSHUB_TOKEN=\"your_token\"\n",
    "# export MLFLOW_TRACKING_URI=\"https://mlflow.dagshub.com/your_username/your_repo_name/...\"\n",
    "\n",
    "# Check if environment variables are set\n",
    "if not os.environ.get(\"DAGSHUB_USER\") or not os.environ.get(\"DAGSHUB_TOKEN\") or not os.environ.get(\"MLFLOW_TRACKING_URI\"):\n",
    "    print(\"Error: DagsHub credentials (DAGSHUB_USER, DAGSHUB_TOKEN) or MLFLOW_TRACKING_URI are not set.\")\n",
    "    print(\"Please set them as environment variables in your terminal before running this notebook.\")\n",
    "    # Exit the notebook if credentials are not set\n",
    "    # exit() # Uncomment to stop execution if variables are not set\n",
    "else:\n",
    "    print(f\"MLflow tracking URI set to: {os.environ['MLFLOW_TRACKING_URI']}\")\n",
    "\n",
    "# Initialize DagsHub and MLflow\n",
    "try:\n",
    "    dagshub.init(repo_owner='sandhya-bdb', repo_name='mlflow_dagshub_new', mlflow=True)\n",
    "    # Set the experiment name\n",
    "    mlflow.set_experiment(\"Beverage Price Range Prediction\")\n",
    "    print(\"DagsHub and MLflow initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing DagsHub/MLflow: {e}\")\n",
    "    # exit() # Uncomment to stop execution if initialization fails\n",
    "\n",
    "# --- Constants and File Paths ---\n",
    "DATA_FILE = 'survey_results_op.csv'\n",
    "TARGET_COLUMN = 'price_range'\n",
    "RESPONDENT_ID_COLUMN = 'respondent_id'\n",
    "\n",
    "# --- Filenames for saved artifacts ---\n",
    "BEST_MODEL_FILENAME = 'best_price_range_model.pkl'\n",
    "TARGET_ENCODER_FILENAME = 'price_range_target_encoder.pkl'\n",
    "LABEL_ENCODERS_FILENAME = 'label_encoders.pkl' # Dictionary of encoders\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Load Data\n",
    "\n",
    "# %%\n",
    "try:\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "    print(f\"Dataset '{DATA_FILE}' loaded successfully. Shape: {df.shape}\")\n",
    "    print(\"First 5 rows of the dataset:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{DATA_FILE}' was not found. Please ensure it's in the same directory as the notebook or provide the full path.\")\n",
    "    exit() # Exit if data loading fails\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Data Preprocessing and Feature Engineering\n",
    "\n",
    "# %%\n",
    "# Separate target variable\n",
    "if TARGET_COLUMN not in df.columns:\n",
    "    print(f\"Error: Target column '{TARGET_COLUMN}' not found in the dataset.\")\n",
    "    exit()\n",
    "X = df.drop(columns=[TARGET_COLUMN, RESPONDENT_ID_COLUMN], errors='ignore') # Ignore respondent_id if it exists\n",
    "y = df[TARGET_COLUMN]\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}, Target shape: {y.shape}\")\n",
    "\n",
    "# Define columns for different preprocessing steps\n",
    "label_encode_cols_definitions = [\n",
    "    'age_group',\n",
    "    'income_levels',\n",
    "    'health_concerns',\n",
    "    'consume_frequency(weekly)',\n",
    "    'preferable_consumption_size'\n",
    "]\n",
    "# Filter to include only columns that actually exist in X\n",
    "label_encode_cols = [col for col in label_encode_cols_definitions if col in X.columns]\n",
    "\n",
    "# Identify categorical columns that are NOT in label_encode_cols for One-Hot Encoding\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "one_hot_encode_cols = [col for col in categorical_cols if col not in label_encode_cols]\n",
    "\n",
    "print(f\"\\nColumns for Label Encoding: {label_encode_cols}\")\n",
    "print(f\"Columns for One-Hot Encoding: {one_hot_encode_cols}\")\n",
    "\n",
    "# --- Apply Label Encoding ---\n",
    "label_encoders = {} # Dictionary to store fitted LabelEncoders for later use\n",
    "for col in label_encode_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    label_encoders[col] = le # Save the fitted encoder\n",
    "    print(f\"Label encoded '{col}'. Classes: {le.classes_}\")\n",
    "\n",
    "# --- Apply One-Hot Encoding ---\n",
    "if one_hot_encode_cols:\n",
    "    X = pd.get_dummies(X, columns=one_hot_encode_cols, drop_first=True)\n",
    "    print(f\"\\nApplied One-Hot Encoding. New X shape: {X.shape}\")\n",
    "else:\n",
    "    print(\"\\nNo columns for One-Hot Encoding were found or specified.\")\n",
    "\n",
    "# Convert boolean columns created by get_dummies to integers (optional but good practice)\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'bool':\n",
    "        X[col] = X[col].astype(int)\n",
    "        print(f\"Converted boolean column '{col}' to int.\")\n",
    "\n",
    "# --- Encode Target Variable ---\n",
    "target_encoder = LabelEncoder()\n",
    "y_encoded = target_encoder.fit_transform(y)\n",
    "print(f\"\\nTarget variable '{TARGET_COLUMN}' encoded. Original classes: {target_encoder.classes_}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Data Splitting\n",
    "\n",
    "# %%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.25, random_state=42)\n",
    "print(f\"Data split into training and testing sets.\")\n",
    "print(f\"Training set features shape: {X_train.shape}\")\n",
    "print(f\"Testing set features shape: {X_test.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Model Training, Evaluation, and MLflow Logging\n",
    "\n",
    "# %%\n",
    "# Define models\n",
    "models = {\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Support Vector Machine (SVM)\": SVC(probability=True, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"Light Gradient Boosting Machine (LightGBM)\": LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = -1\n",
    "best_model_name = \"\"\n",
    "best_run_id = \"\" # To store the MLflow run ID of the best model\n",
    "\n",
    "print(\"\\n--- Starting Model Training and Evaluation ---\")\n",
    "\n",
    "# Iterate through each model\n",
    "for model_name, model_instance in models.items():\n",
    "    # Start an MLflow run for the current model\n",
    "    with mlflow.start_run(run_name=f\"Train_{model_name}\") as run: # Capture run object\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        current_run_id = run.info.run_id # Get the ID of the current run\n",
    "        print(f\"MLflow Run ID: {current_run_id}\")\n",
    "\n",
    "        # Log hyperparameters of the current model\n",
    "        mlflow.log_params(model_instance.get_params())\n",
    "\n",
    "        # Train the model on the training data\n",
    "        model_instance.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        y_pred = model_instance.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        class_report_dict = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        class_report_str = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"test_set_accuracy\", accuracy)\n",
    "\n",
    "        # Log classification report metrics\n",
    "        for class_label, metrics in class_report_dict.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                mlflow.log_metric(f\"precision_{class_label}\", metrics.get(\"precision\", 0))\n",
    "                mlflow.log_metric(f\"recall_{class_label}\", metrics.get(\"recall\", 0))\n",
    "                mlflow.log_metric(f\"f1-score_{class_label}\", metrics.get(\"f1-score\", 0))\n",
    "            elif isinstance(metrics, (int, float)) and class_label != 'accuracy':\n",
    "                 mlflow.log_metric(f\"{class_label}_avg\", metrics)\n",
    "\n",
    "        # Log classification report as a text artifact\n",
    "        report_filename = f\"{model_name}_classification_report.txt\"\n",
    "        with open(report_filename, \"w\") as f:\n",
    "            f.write(class_report_str)\n",
    "        mlflow.log_artifact(report_filename)\n",
    "        os.remove(report_filename) # Clean up local file\n",
    "\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Classification Report:\\n{class_report_str}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        # Track the best model based on accuracy\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model_instance\n",
    "            best_model_name = model_name\n",
    "            best_run_id = current_run_id # Store the MLflow run ID\n",
    "\n",
    "print(\"\\n--- Model Training and Evaluation Complete ---\")\n",
    "print(f\"Best model found: '{best_model_name}' with accuracy {best_accuracy:.4f} (MLflow Run ID: {best_run_id})\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Save Best Model and Preprocessing Artifacts (.pkl files)\n",
    "\n",
    "# %%\n",
    "print(\"\\n--- Saving Best Model and Preprocessing Artifacts ---\")\n",
    "\n",
    "# Save the best performing model using joblib\n",
    "try:\n",
    "    joblib.dump(best_model, BEST_MODEL_FILENAME)\n",
    "    print(f\"Best model ('{best_model_name}') saved to '{BEST_MODEL_FILENAME}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving best model: {e}\")\n",
    "\n",
    "# Save the target encoder using joblib\n",
    "try:\n",
    "    joblib.dump(target_encoder, TARGET_ENCODER_FILENAME)\n",
    "    print(f\"Target encoder saved to '{TARGET_ENCODER_FILENAME}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving target encoder: {e}\")\n",
    "\n",
    "# Save the dictionary of label encoders using joblib\n",
    "try:\n",
    "    joblib.dump(label_encoders, LABEL_ENCODERS_FILENAME)\n",
    "    print(f\"Label encoders dictionary saved to '{LABEL_ENCODERS_FILENAME}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving label encoders: {e}\")\n",
    "\n",
    "# --- Log Saved Artifacts to MLflow ---\n",
    "# It's good practice to log the final artifacts to the MLflow run of the best model.\n",
    "# We can do this by reopening the run or by logging to the currently active (if any) or experiment.\n",
    "# Since the loop has finished, we can retrieve the best model's run and log.\n",
    "# However, a simpler way is to log them as general artifacts for the experiment.\n",
    "# For best practice, we'll log them to the MLflow run associated with the best model.\n",
    "\n",
    "if best_run_id: # Ensure we found a best model run\n",
    "    try:\n",
    "        print(f\"Logging saved artifacts to MLflow run ID: {best_run_id}\")\n",
    "        # This requires that the run is still active or we fetch it.\n",
    "        # A common way is to log during the run. Since we finished the run,\n",
    "        # we can log them as general artifacts under the experiment.\n",
    "        # If you MUST log them to that specific run, you'd need to use mlflow.search_runs\n",
    "        # to find the run and then use mlflow.artifacts.log_artifacts or similar.\n",
    "\n",
    "        # For simplicity and to ensure they are available, we'll log them generally\n",
    "        # If you need them associated with the *specific* run, modify the loop to save/log INSIDE it\n",
    "        # when accuracy > best_accuracy.\n",
    "        # Let's log them to the experiment level for now.\n",
    "        mlflow.log_artifact(BEST_MODEL_FILENAME)\n",
    "        mlflow.log_artifact(TARGET_ENCODER_FILENAME)\n",
    "        mlflow.log_artifact(LABEL_ENCODERS_FILENAME)\n",
    "        print(\"Saved artifacts logged as MLflow artifacts.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error logging artifacts to MLflow: {e}\")\n",
    "else:\n",
    "    print(\"No best model found or run ID not captured, skipping artifact logging to MLflow.\")\n",
    "\n",
    "\n",
    "# Log metadata about the best model found as parameters\n",
    "mlflow.log_param(\"best_model_name\", best_model_name)\n",
    "mlflow.log_param(\"best_model_accuracy\", best_accuracy)\n",
    "mlflow.log_param(\"best_model_run_id\", best_run_id) # Log the run ID of the best model\n",
    "\n",
    "print(\"\\nMLflow tracking and artifact saving complete.\")\n",
    "print(f\"View results in MLflow UI by running 'mlflow ui' in your terminal.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Next Steps\n",
    "#\n",
    "# You can now use the saved files (`best_price_range_model.pkl`, `price_range_target_encoder.pkl`, `label_encoders.pkl`) in your Streamlit application to perform predictions.\n",
    "#\n",
    "# **Remember to:**\n",
    "# 1.  Place these `.pkl` files in the same directory as your `app.py` file, or specify their correct paths in `app.py`.\n",
    "# 2.  Ensure the preprocessing logic in `app.py` exactly mirrors what was done in this notebook for accurate predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d7874-4fbe-406a-be44-66354b6bf625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
